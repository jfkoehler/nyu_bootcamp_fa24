{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Hypothesis Testing\n",
    "\n",
    "**OBJECTIVES**\n",
    "\n",
    "- Review confidence intervals\n",
    "- Review standard error of the mean\n",
    "- Introduce Hypothesis Testing\n",
    " - Hypothesis test with one sample\n",
    " - Difference in two samples\n",
    " - Difference in multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz Review\n",
    "\n",
    "Using the `titanic` data, determine which features seem to discriminate well between passengers who survived and those that did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Suppose we have two distributions on different domains from which we would like to compare scores.  \n",
    "- An English Class has test scores normally distributed with mean 95 and standard deviation 5.\n",
    "\n",
    "- A Mathematics Class has test scores normally distributed with mean 80 and standard deviation 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math class\n",
    "math_class = stats.norm(loc = 80, scale = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "plt.hist(math_class.rvs(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english scores\n",
    "english_class = stats.norm(loc = 95, scale = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe\n",
    "tests_df = pd.DataFrame({'math': math_class.rvs(1000), 'english': english_class.rvs(1000)})\n",
    "tests_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histograms together\n",
    "plt.hist(tests_df['math'])\n",
    "plt.hist(tests_df['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem: Student A -- 82 in math How many std's away from the mean is 82???\n",
    "#.        Student B -- 97 in English\n",
    "#Who did better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Standardizer`\n",
    "\n",
    "The work of standardizing our data is extremely important for many models.  To get a feel for an important library, your task is to build a `Standardizer` class that has two methods:\n",
    "\n",
    "```python\n",
    ".fit()\n",
    ".transform()\n",
    "```\n",
    "\n",
    "When the `.fit` method is called, you will learn the mean and standard deviation of the data.  Upon learning these, assign them to the attributes `.mean_` and `.scale_`.  Then, use the `.transform` method to actually transform the data.  Demonstrate its use with the `tests_df`.  Note, you will need to call the `.fit` method prior to the `.transform`.  As a bonus, try adding an error message that warns the user when calling `fit` prior to calling `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.scale_ = None\n",
    "        \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def transform():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the polls data\n",
    "polls = pd.read_csv('https://raw.githubusercontent.com/jfkoehler/nyu_bootcamp_fa24/refs/heads/main/data/polls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a peek\n",
    "polls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals\n",
    "\n",
    "$$\\mu \\pm t_{1 - \\alpha / 2} \\times \\frac{s}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\alpha$: significance level -- we determine this\n",
    "- *t*: t-score -- we look this up\n",
    "- $\\mu$: we get this from the data\n",
    "- $s$: we get this from the data **NOTE**: This is different than a population standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine the first question data\n",
    "q1 = polls['p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine degrees of freedom\n",
    "#i.e. length - 1\n",
    "dof = len(q1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look up test statistic\n",
    "#we need our alpha and dof\n",
    "#where do we bound 97.5% of our data\n",
    "t_stat = stats.t.ppf(1 - 0.05/2, dof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute sample standard deviation\n",
    "s = np.std(q1, ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample size\n",
    "n = len(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute upper limit\n",
    "upper = q1.mean() + t_stat*s/np.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the lower bound\n",
    "lower = q1.mean() - t_stat*s/np.sqrt(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print it\n",
    "(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use scipy\n",
    "#1 - alpha\n",
    "#dof\n",
    "#sem\n",
    "#(1 - alpha, dof, mean, sem)\n",
    "stats.t.interval(.95, n - 1, np.mean(q1), stats.sem(q1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot it\n",
    "#take 500 samples of size 7 from poll 1, find mean, kde of the results\n",
    "sample_means = [q1.sample(20).mean() for _ in range(5000)]\n",
    "sns.displot(sample_means, kind = 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "- Find the 95% confidence interval for the second poll\n",
    "- Compare the two intervals, is there much overlap?  What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval for Difference in Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statsmodels imports\n",
    "from statsmodels.stats.weightstats import CompareMeans, DescrStatsW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our objects polls are DescrStatsWeights\n",
    "#compare means of these\n",
    "dq1 = DescrStatsW(q1)\n",
    "dq2 = DescrStatsW(q2)\n",
    "c = CompareMeans(dq1, dq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% confidence interval -- represents the difference between \n",
    "c.tconfint_diff(.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs Data\n",
    "\n",
    "The data below is a sample of job postings from New York City.  We want to investigate the lower and upper bound columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "jobs = pd.read_csv('https://raw.githubusercontent.com/jfkoehler/nyu_bootcamp_fa24/refs/heads/main/data/jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salary from\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin of Error\n",
    "\n",
    "Now, the question is to build a confidence interval that achieves a given amount of error.\n",
    "\n",
    "$$error = z_{\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM**\n",
    "\n",
    "What is the minimum sample size necessary to estimate the upper salary range with 95% confidence within \\$3000?\n",
    "\n",
    "- need $z$-score: 1.96\n",
    "- E: 3000\n",
    "- $\\sigma$: `np.std(jobs['salary_to'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for $500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Significance\n",
    "\n",
    "Now that we've tackled confidence intervals, let's wrap up with a final test for significance.  With a Hypothesis Test, the first step is declaring a null and alternative hypothesis.  Typically, this will be an assumption of no difference.\n",
    "\n",
    "$$H_0: \\text{Null Hypothesis}$$\n",
    "$$H_a: \\text{Alternative Hypothesis}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, our data below have to do with a reading intervention and assessment after the fact.  Our null hypothesis will be:\n",
    "\n",
    "$$H_0: \\mu_1 = \\mu_2$$\n",
    "$$H_a: \\mu_1 \\neq \\mu_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "reading = pd.read_csv('https://raw.githubusercontent.com/jfkoehler/nyu_bootcamp_fa24/refs/heads/main/data/DRP.csv')\n",
    "reading.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions of groups\n",
    "sns.displot(x = 'drp', hue = 'group', data = reading, kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our hypothesis test, we need two things:\n",
    "\n",
    "- Null and alternative hypothesis\n",
    "\n",
    "$$H_0: \\mu_t = \\mu_c $$\n",
    "$$H_a: \\mu_t \\neq \\mu_c $$\n",
    "- Significance Level\n",
    "\n",
    " - $\\alpha = 0.05$\n",
    "Just like before, we will set a tolerance for rejecting the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the groups\n",
    "treatment = reading.loc[reading['g'] == 0]['drp']\n",
    "control = reading.loc[reading['g'] == 1]['drp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the test\n",
    "stats.ttest_ind(treatment, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha at 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPOSE WE WANT TO TEST IF INTERVENTION MADE SCORES HIGHER\n",
    "\n",
    "$$H_0: \\mu_0 = \\mu_1$$\n",
    "$$H_1: \\mu_0 < \\mu_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha at 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_score, p = stats.ttest_ind(treatment, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEMS**\n",
    "\n",
    "1. Given the `mileage` dataset, test the claim on the cars sticker that the average mpg for city driving is 30 mpg.\n",
    "\n",
    "2. If we increase our food intake, we generally gain weight.  In one study, researchers fed 16 non-obese adults, age 25-36 1000 excess calories a day.  According to theory, 3500 extra calories will translate into a weight gain of 1 point, therefore we expect each of the subjects to gain 16 pounds.  the `wtgain` dataset contains the before and after eight week period gains.\n",
    "\n",
    "  - Create a new column to represent the weight change of each subject.\n",
    "  - Find the mean and standard deviation for the change.\n",
    "  - Determine the 95% confidence interval for weight change and interpret in complete sentences.\n",
    "  - Test the null hypothesis that the mean weight gain is 16 lbs.  What do you conclude?\n",
    "  \n",
    "3. Insurance adjusters are concerned about the high estimates they are receiving from Jocko's Garage.  To see if the estimates are unreasonably high, each of 10 damaged cars was take to Jocko's and to another garage and the estimates were recorded in the `jocko.csv` file.  \n",
    "\n",
    "  - Create a new column that represents the difference in prices from the two garages. Find the mean and standard deviation of the difference.\n",
    "  - Test the null hypothesis that there is no difference between the estimates at the 0.05 significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
